{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tiktoken\n",
    "import gzip\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a data scientist specializing in grouping plant biological interactions. \n",
    "Your task is to cluster similar edges while strictly adhering to the following guidelines: \n",
    "1. Exact Phrase Matching Matters: \n",
    "1.1. Consider the Entire Phrase: Treat each edge as a single, whole phrase, including all key biological terms and any bracketed text. \n",
    "1.2. Ignore Minor Surface Differences: Variations such as letter casing, spacing, punctuation, standard abbreviations, or singular vs. plural forms do not create new or separate edges. \n",
    "2. Strict (100%) Key Term Separation: If an edge has a different key biological term, it MUST GO into a separate cluster. \n",
    "3. Sub-identifier Separation: If an edge differs by any numeric value, sub-identifier, or qualifier, it MUST be placed in a separate cluster. \n",
    "4. Avoid False Similarity: DO NOT cluster edges together simply because they share a common word; group them only if their overall semantic meaning is the same (e.g., \"[protein] interacts with [metabolite]\" and \"[protein] is interacting with [metabolite]\"). \n",
    "5. Extra Descriptor Differentiation: If an edge has an extra descriptor that changes its meaning, do not group it with others, unless the extra descriptor is a synonym. \n",
    "6. Strict Synonym/Near-Synonym Grouping: Group edges together only if they refer to the exact same biological structure, process, or concept. \n",
    "7. Maintain 100% Precision: If there is any doubt whether two edges are the same, place them in separate clusters.\n",
    "8. Preserve Original Data: DO NOT introduce new items, create duplicates, or omit any edge from your final output. \n",
    "9. Output Format: Always return results in valid JSON format using the given key. The output must be a list of lists, where each cluster is its own list. The total number of output entries must match the input entries. \n",
    "10. Choose Cluster Representative: \n",
    "10.1. For every cluster containing more than one edge, YOU MUST choose exactly one representative edge. \n",
    "10.2 The representative edge must be the most appropriate and easy-to-understand version. It should also correspond to the best biological meaning of the edge, so \"supplies\" rather than \"provides means to sustain\" should be the representative edge, since it is more specific and easier to understand.\n",
    "10.3. Enclose the representative edge with '**' (double asterisks) in the output. \n",
    "10.4. Failure to appoint a cluster representative invalidates the outputâ€”this is critical. \n",
    "11. The categories enclosed by \"[]\" denotes the node types; in edge clustering, edges absolutely must only be clustered together if both node types match. Edges are directional, so \"[gene] coincided with [phenotype]\" cannot not be clustered with \"[phenotype] coincided with [gene]\".\n",
    "12. Only output clusters with more than one member, single member clusters should be omitted.\n",
    "\n",
    "\n",
    "ALWAYS KEEP IN MIND: \n",
    "BE VERY SURE that you do not add any new terms to the output, that the output entries EXACTLY match the input entries, that the output format is correct, and that the output is a list of lists. \n",
    "Edges are directional, so \"[gene] coincided with [phenotype]\" cannot not be clustered with \"[phenotype] coincided with [gene]\".\n",
    "You cannot cluster terms that do not have the exact same node types, this is a rule, not a suggestion.\n",
    "DO NOT forget to select cluster representatives by enclosing one edge in each multi-element cluster with '**'. \n",
    "\n",
    "Example input 1: \"21329\": ['[protein] could interact with [metabolite]', '[protein] could potentially interact with [metabolite]', '[protein] might interact with [metabolite]', '[protein] most definitely interacts with [metabolite]', '[metabolite] might interact with [protein]']\n",
    "\n",
    "Example output 1: {\"21329\": [[\"[protein] could interact with [metabolite]\", \"[protein] could potentially interact with [metabolite]\", \"**[protein] might interact with [metabolite]**\"], [\"**[metabolite] might interact with [protein]**\"]]}\n",
    "\n",
    "Example input 2: \"211\": ['[organism] is beneficial for [process]', '[organism] is beneficial to [organism]', '[organism] beneficial for [process]', '[organism] beneficial for [organism]', '[organism] included plant beneficial microorganisms like [organism]', '[organism] beneficial to [organism]', '[organism] provide beneficial functions to [organism]', '[organism] may be beneficial to [organism]', '[organism] is beneficial for [organism]', '[organism] beneficial to [organ]', '[organism] are beneficial to [organism]', '[organism] confers beneficial traits to [organism]', '[organism] healthy to [organism]', '[organism] useful to [organism]', '[organism] beneficial for [environment]', '[organism] colonization is often of benefit to [organism]', '[interaction] beneficial for [organism]', '[organism] may be beneficial for [organism]', '[organism] provide beneficial services to [organism]']\n",
    "\n",
    "Example output 2: {\"211\": [[\"**[organism] is beneficial for [process]**\", \"[organism] beneficial for [process]\"], [\"**[organism] is beneficial to [organism]**\", \"[organism] beneficial to [organism]\", \"[organism] are beneficial to [organism]\", \"[organism] is beneficial for [organism]\", \"[organism] beneficial for [organism]\"]]}\n",
    "\n",
    "Example input 3: \"213\": ['[gene] were co-repressed or co-activated by [gene]', '[gene] selective induction/repression of [gene]', '[gene] may be repressed by [gene]', '[gene] could be repressed by [gene]', '[gene] can be repressed by interacting with [gene]', '[gene] are induced or repressed by [gene]', '[gene] repressible by [mutant]', '[gene] were induced or repressed by [other]', '[gene] were induced or repressed by [gene]', '[gene] can be repressed by inducing [gene]', '[gene] are crucial in expressing or repressing [gene]', '[gene] inducing or repressing [gene]', '[gene] induced or repressed by [gene]', '[gene] activated or repressed by [gene]']\n",
    "\n",
    "Example output 3: {\"213\": [[\"**[gene] may be repressed by [gene]**\", \"[gene] could be repressed by [gene]\"], [\"**[gene] are induced or repressed by [gene]**\", \"[gene] were induced or repressed by [gene]\", \"[gene] induced or repressed by [gene]\", \"[gene] inducing or repressing [gene]\", \"[gene] activated or repressed by [gene]\"]]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.4 For single-element clusters, no representative modification is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(array_of_strings):\n",
    "    \"\"\"Prepare GO terms data for embedding.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    input_data = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    for edge in array_of_strings:\n",
    "        text = edge\n",
    "        id = edge.split(\":\")[0]\n",
    "        n_tokens = len(encoding.encode(text))\n",
    "        total_tokens += n_tokens\n",
    "        \n",
    "        input_data.append({\n",
    "            'id': id,\n",
    "            'text': text,\n",
    "            'n_tokens': n_tokens\n",
    "        })\n",
    "\n",
    "    estimated_cost = (total_tokens / 1000000) * 0.1\n",
    "    print(f\"Total number of submissions: {len(input_data)}\")\n",
    "    print(f\"Total input tokens: {total_tokens}\")\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "\n",
    "def create_batch_file(completion_data, query_dir, timestamp, system_prompt, model=\"o3-mini\", model_prefix=None, options=None, chunk_number=None):\n",
    "    \"\"\"Create JSONL file for batch completion requests.\n",
    "    \n",
    "    Args:\n",
    "        completion_data (list): List of dictionaries containing request data\n",
    "        query_dir (str): Directory to save the batch file\n",
    "        timestamp (str): Timestamp for the filename\n",
    "        system_prompt (str): System prompt to use for all requests\n",
    "        model (str): Model to use for completion requests (default: \"o3-mini\")\n",
    "        model_prefix (str): Prefix for the model name\n",
    "        options (dict): Additional model options like temperature or reasoning_effort\n",
    "        chunk_number (optional): Chunk number if processing in batches\n",
    "    \"\"\"\n",
    "\n",
    "    if options is None:\n",
    "        #exit and ask user to provide options\n",
    "        raise ValueError(\"Options are required\")\n",
    "\n",
    "    # Create filename based on model type\n",
    "    if model_prefix is None:\n",
    "        model_prefix = model\n",
    "    batch_fname = f'{model_prefix}_completion_requests_{timestamp}'\n",
    "    if chunk_number is not None:\n",
    "        batch_fname += f'_chunk_{chunk_number}'\n",
    "    batch_fname += '.jsonl'\n",
    "    batch_file_path = os.path.join(query_dir, batch_fname)\n",
    "\n",
    "    with open(batch_file_path, 'w') as f:\n",
    "        for item in completion_data:\n",
    "            request = {\n",
    "                \"custom_id\": item['id'],\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": model,\n",
    "                    \"response_format\": {\"type\": \"json_object\"},\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_prompt\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": item['text']\n",
    "                        }\n",
    "                    ],\n",
    "                    **options\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(request) + '\\n')\n",
    "    \n",
    "    return batch_file_path\n",
    "\n",
    "\n",
    "def submit_batch_job(client, batch_file_path, description):\n",
    "    \"\"\"Submit batch job to OpenAI API.\"\"\"\n",
    "    \n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(batch_file_path, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "    #check if file is uploaded successfully\n",
    "    while batch_input_file.status != \"processed\":\n",
    "        time.sleep(1)\n",
    "    \n",
    "    batch = client.batches.create(\n",
    "        input_file_id=batch_input_file.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"description\": description}\n",
    "    )\n",
    "    print(f\"batch created\\n{batch.id}\")\n",
    "    return batch\n",
    "\n",
    "\n",
    "def create_multiple_batch_files(client, input_array, timestamp, query_dir, system_prompt, model, model_prefix, options, max_batch_size=50_000, description=\"Edge embeddings batch job\"):\n",
    "    \"\"\"Create multiple batch files for embedding.\"\"\"\n",
    "    print(f\"Model: {model}, Model prefix: {model_prefix}, Options: {options}\")\n",
    "    #split embeddings_array into chunks of max_batch_size\n",
    "    chunks = [input_array[i:i+max_batch_size] for i in range(0, len(input_array), max_batch_size)]\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    batch_ids = []\n",
    "    \n",
    "    batch_file_paths = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        batch_file_path = create_batch_file(chunk, query_dir, timestamp, system_prompt, model = model, model_prefix=model_prefix, options=options, chunk_number=i)\n",
    "        batch_file_paths.append(batch_file_path)\n",
    "        batch= submit_batch_job(client, batch_file_path, description)\n",
    "        batch_ids.append(batch.id)\n",
    "        \n",
    "\n",
    "    #save the batch_ids and batch_file_paths to a csv file\n",
    "    batch_info = pd.DataFrame({\"batch_id\": batch_ids, \"batch_file_path\": batch_file_paths})\n",
    "    batch_info.to_csv(os.path.join(query_dir, f\"batch_info_{timestamp}.csv\"), index=False)\n",
    "        \n",
    "    return batch_ids, batch_file_paths\n",
    "\n",
    "\n",
    "def wait_for_completion(client, batch_id, initial_pause=4):\n",
    "    \"\"\"Wait for batch job completion with exponential backoff.\"\"\"\n",
    "    pause = initial_pause\n",
    "    completed = False\n",
    "    while not completed:\n",
    "        batch = client.batches.retrieve(batch_id)\n",
    "        print(batch.status)\n",
    "        \n",
    "        if batch.status == \"completed\":\n",
    "            print(\"Batch completed\")\n",
    "            completed = True\n",
    "            return batch\n",
    "        \n",
    "        print(f\"Batch not completed, pausing for {pause} seconds\")\n",
    "        time.sleep(pause)\n",
    "        pause *= 2\n",
    "\n",
    "\n",
    "def wait_for_completion_for_multiple_batches(client, batch_ids, initial_pause=4):\n",
    "    \"\"\"Wait for completion of multiple batch jobs with exponential backoff.\"\"\"\n",
    "    pause = initial_pause\n",
    "    all_completed = False\n",
    "    #mark all batches as not completed\n",
    "    batch_statuses_dict = {batch_id: \"bla\" for batch_id in batch_ids}\n",
    "    batches = []\n",
    "    while all_completed == False:\n",
    "        for batch_id in batch_ids:\n",
    "            if batch_statuses_dict[batch_id] != \"completed\":\n",
    "                batch = client.batches.retrieve(batch_id)\n",
    "                if batch.status == \"completed\":\n",
    "                    batch_statuses_dict[batch_id] = batch.status\n",
    "                    batches.append(batch)\n",
    "                else:\n",
    "                    batch_statuses_dict[batch_id] = batch.status\n",
    "                \n",
    "        #check if all batches are completed\n",
    "        if all(status == \"completed\" for status in batch_statuses_dict.values()):\n",
    "            print(\"All batches completed\")\n",
    "            print(batch_statuses_dict)\n",
    "            all_completed = True\n",
    "            return batches\n",
    "        \n",
    "        \n",
    "        print(batch_statuses_dict)\n",
    "        \n",
    "        if pause > 300:\n",
    "            pause = 300\n",
    "            \n",
    "        print(f\"Batch not completed, pausing for {pause} seconds\")\n",
    "        time.sleep(pause)\n",
    "        pause *= 2\n",
    "\n",
    "\n",
    "def process_output(client, batch, output_dir, timestamp, batch_file_path, chunk_number=None, model_prefix=\"completion\"):\n",
    "    \"\"\"Download jsonl files from openai and save them to the output_dir\"\"\"\n",
    "\n",
    "    # Save output\n",
    "    output_fname = f'embedding_output_{timestamp}'\n",
    "    if chunk_number is not None:\n",
    "        output_fname = f'{model_prefix}_output_{timestamp}_chunk_{chunk_number}.jsonl'\n",
    "    output_path = os.path.join(output_dir, output_fname)\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"output file already exists, skipping {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "    batch_output_file = client.files.content(batch.output_file_id)\n",
    "    content = batch_output_file.text\n",
    "    split_content = content.split(\"\\n\")[:-1]\n",
    "    \n",
    "    print(f\"batch output file saved to {output_path}\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        for line in split_content:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "    # Save batch info\n",
    "    batch_info = {\n",
    "        \"batch_file_path\": batch_file_path,\n",
    "        \"batch_id\": batch.id,\n",
    "        \"output_file_id\": batch.output_file_id\n",
    "    }\n",
    "    \n",
    "    log_fname = f'{model_prefix}_log_{timestamp}.json'\n",
    "    if chunk_number is not None:\n",
    "        log_fname = f'{model_prefix}_log_{timestamp}_chunk_{chunk_number}.json'\n",
    "    log_path = os.path.join(output_dir, log_fname)\n",
    "    \n",
    "    with open(log_path, 'w') as f:\n",
    "        json.dump(batch_info, f, indent=4)\n",
    "\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "def process_output_for_multiple_batches(client, batch_ids, batch_file_paths, output_dir, timestamp, model_prefix):\n",
    "    \"\"\"Calls process_output for each batch and returns a list of output paths\"\"\"\n",
    "    i = 0\n",
    "    output_paths = []\n",
    "    for batch_id, batch_file_path in zip(batch_ids, batch_file_paths):\n",
    "        batch = client.batches.retrieve(batch_id)\n",
    "        output_path = process_output(client, batch, output_dir, timestamp, batch_file_path, chunk_number=i, model_prefix=model_prefix)\n",
    "\n",
    "        i += 1\n",
    "        output_paths.append(output_path)\n",
    "    \n",
    "    return output_paths\n",
    "\n",
    "\n",
    "def load_output_jsonl(file_path, client=None, api_key=None):\n",
    "    \"\"\"Load output from JSONL file.\"\"\"\n",
    "    with open(file_path, 'rt') as f:  # Changed to gzip.open with text mode\n",
    "        data = []\n",
    "        for line in f:\n",
    "\n",
    "                json_obj = json.loads(line)\n",
    "                embedding = json_obj['response']['body']['choices'][0]['message']['content']\n",
    "                data.append({\n",
    "                    'id': json_obj['custom_id'],\n",
    "                    'embedding': embedding\n",
    "                })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_output_for_multiple_batches(output_paths, client=None, api_key=None):\n",
    "    \"\"\"Calls load_output_jsonl for each output path and concatenates the results.\"\"\"\n",
    "    output_dfs = pd.concat([load_output_jsonl(output_path, client, api_key) for output_path in output_paths])\n",
    "    return output_dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of clusters: 116976\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cluster_df = pd.read_parquet('/home/mads/connectome/data/embeddings/edge_embeddings/clustering/edge_clusters_6_max_clust_30.parquet')\n",
    "print(f\"Total number of clusters: {len(cluster_df)}\")\n",
    "#search the lists in ids_in_cluster for strings containing \"interacts with\"\n",
    "#cluster_df = cluster_df[cluster_df['ids_in_cluster'].apply(lambda x: any(\"interact\" in item.lower() for item in x))]\n",
    "cluster_df = cluster_df[cluster_df.cluster_size > 1]\n",
    "\n",
    "#pick 1000 random rows from cluster_df, set seed to 42\n",
    "#cluster_df = cluster_df.sample(n=1000, random_state=42)\n",
    "\n",
    "\n",
    "#create the input string. the string should be \"index: the list of edges as a string\"\n",
    "cluster_df[\"input\"] = cluster_df.apply(lambda row: f\"{row.name}: {row['ids_in_cluster']}\", axis=1)\n",
    "inputs = cluster_df[\"input\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All batches completed\n",
      "{'batch_67ea875e118c81908830f7aa72a9c11f': 'completed', 'batch_67ea876e197c8190b1b3bda736767e16': 'completed', 'batch_67ea877f5d70819088cf375122f176c4': 'completed', 'batch_67ea878c31688190be47dbf1a5ca30c2': 'completed'}\n",
      "output file already exists, skipping /home/mads/connectome/data/predictions/output/v8_full_output_20250331_141507_chunk_0.jsonl\n",
      "output file already exists, skipping /home/mads/connectome/data/predictions/output/v8_full_output_20250331_141507_chunk_1.jsonl\n",
      "output file already exists, skipping /home/mads/connectome/data/predictions/output/v8_full_output_20250331_141507_chunk_2.jsonl\n",
      "output file already exists, skipping /home/mads/connectome/data/predictions/output/v8_full_output_20250331_141507_chunk_3.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Load API key for OpenAI\n",
    "with open(\"/home/mads/connectome/api_key.txt\", \"r\") as f:\n",
    "    api_key = f.read()\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "query_dir = \"/home/mads/connectome/data/predictions/queries\"\n",
    "# Check if the query directory exists; if not, create it\n",
    "if not os.path.exists(query_dir):\n",
    "    os.makedirs(query_dir)\n",
    "\n",
    "output_dir = \"/home/mads/connectome/data/predictions/output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Prepare embedding data from unique connection types\n",
    "run_from_scratch = input(\"Do you want to run from scratch? (y/n): \")\n",
    "#assert that run_from_scratch is either \"y\" or \"n\", and give a message if it is not\n",
    "assert run_from_scratch in [\"y\", \"n\"], \"run_from_scratch must be either 'y' or 'n'\"\n",
    "batch_size = 25_000\n",
    "\n",
    "model_prefix = \"v8_full\"\n",
    "#model = \"ft:gpt-4o-mini-2024-07-18:mutwil-lab:4omini-v9-train-1306-test-78:B9u5DUsf\"\n",
    "model = \"ft:gpt-4o-mini-2024-07-18:mutwil-lab:4omini-v8-train-1306-test-78:B9DybEeH\"\n",
    "options = {\"temperature\": 0, \"max_tokens\": 1000}\n",
    "description = \"Edge disambiguation\"\n",
    "\n",
    "\n",
    "if run_from_scratch == \"y\":\n",
    "\n",
    "    # Generate a timestamp for file identification\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    print(f\"Timestamp file identifier: {timestamp}\")\n",
    "\n",
    "    input_array = prepare_data(inputs)\n",
    "\n",
    "    batch_ids, batch_file_paths= create_multiple_batch_files(client = client,\n",
    "                                                                                input_array = input_array, \n",
    "                                                                                timestamp = timestamp, \n",
    "                                                                                query_dir = query_dir, \n",
    "                                                                                system_prompt = system_prompt, \n",
    "                                                                                model = model, \n",
    "                                                                                model_prefix = model_prefix,\n",
    "                                                                                options = options,\n",
    "                                                                                max_batch_size = batch_size,\n",
    "                                                                                description = description)\n",
    "    \n",
    "if run_from_scratch == \"n\":\n",
    "    #load the batch_info from the csv file\n",
    "    #get user input for the timestamp\n",
    "    timestamp = input(\"Enter the timestamp for the run you want to load: \")\n",
    "    batch_info = pd.read_csv(os.path.join(query_dir, f\"batch_info_{timestamp}.csv\"))\n",
    "    batch_ids = batch_info[\"batch_id\"].tolist()\n",
    "    batch_file_paths = batch_info[\"batch_file_path\"].tolist()\n",
    "\n",
    "\n",
    "\n",
    "wait_for_completion_for_multiple_batches(client, batch_ids, initial_pause=4)\n",
    "\n",
    "output_paths = process_output_for_multiple_batches(client, batch_ids, batch_file_paths, output_dir, timestamp, model_prefix)\n",
    "\n",
    "output_df = load_output_for_multiple_batches(output_paths, client, api_key)\n",
    "\n",
    "output_df.to_parquet(f\"/home/mads/connectome/data/predictions/{model_prefix}_output_{timestamp}.parquet\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{\"0\":\"[[\\\"**[organ] correlated with [metabolit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{\"1\":\"[]\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>{\"2\":\"[[\\\"**[organ] leads to [metabolite]**\\\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>{\"3\":\"[[\\\"**[organ] involves [metabolite]**\\\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>{\"4\":\"[[\\\"**[organ] related to [metabolite]**\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23555</th>\n",
       "      <td>116971</td>\n",
       "      <td>{\"116971\":\"[[\\\"**[gene] are classified into [p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23556</th>\n",
       "      <td>116972</td>\n",
       "      <td>{\"116972\":\"[[\\\"**[gene] differ in [protein dom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23557</th>\n",
       "      <td>116973</td>\n",
       "      <td>{\"116973\":\"[[\\\"**[gene] evolved to [protein do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23558</th>\n",
       "      <td>116974</td>\n",
       "      <td>{\"116974\":\"[]\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23559</th>\n",
       "      <td>116975</td>\n",
       "      <td>{\"116975\":\"[[\\\"**[gene] divided into [protein ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98560 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                          embedding\n",
       "0           0  {\"0\":\"[[\\\"**[organ] correlated with [metabolit...\n",
       "1           1                                         {\"1\":\"[]\"}\n",
       "2           2  {\"2\":\"[[\\\"**[organ] leads to [metabolite]**\\\",...\n",
       "3           3  {\"3\":\"[[\\\"**[organ] involves [metabolite]**\\\",...\n",
       "4           4  {\"4\":\"[[\\\"**[organ] related to [metabolite]**\\...\n",
       "...       ...                                                ...\n",
       "23555  116971  {\"116971\":\"[[\\\"**[gene] are classified into [p...\n",
       "23556  116972  {\"116972\":\"[[\\\"**[gene] differ in [protein dom...\n",
       "23557  116973  {\"116973\":\"[[\\\"**[gene] evolved to [protein do...\n",
       "23558  116974                                    {\"116974\":\"[]\"}\n",
       "23559  116975  {\"116975\":\"[[\\\"**[gene] divided into [protein ...\n",
       "\n",
       "[98560 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
