{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tiktoken\n",
    "import gzip\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a data scientist specializing in grouping plant biological interactions. Your task is to cluster similar edges while strictly adhering to the following guidelines:  \n",
    "\n",
    "1. Exact Phrase Matching Matters:   \n",
    "1.1 Consider the Entire Phrase: Treat each edge as a single, whole phrase. This includes all key biological terms and any bracketed text\n",
    "1.2 Ignore Minor Surface Differences: Minor variations such as letter casing (uppercase vs. lowercase), spacing, punctuation, standard abbreviations, or singular vs. plural forms do not create new or separate edges.  \n",
    "\n",
    "2. Strict (100%) Key Term Separation: If an edge has a different key biological term, it MUST GO into a separate cluster.  \n",
    "\n",
    "3. Sub-identifier separation: If an edge differs by any numeric value, sub-identifier, or qualifier, they MUST BE placed in separate clusters.  \n",
    "\n",
    "4. Avoid False Similarity: DO NOT cluster two edges together simply because they share a common word or term if their overall key term or concept is different. You should cluster them together if the semantic meaning is the same, example: \"[protein] interacts with [metabolite]\" and \"[protein] is interacting with [metabolite]\" should be clustered together.\n",
    "\n",
    "5. Extra Descriptor Differentiation: If one edge has an extra descriptor that changes its meaning, do not group them together. However, if the extra descriptor is a synonym, then group them together.\n",
    "\n",
    "6. Strict Synonym/Near-Synonym Grouping: Only group edges together if they refer to the exact same biological structure, process, or concept.  \n",
    "\n",
    "7. Maintain 100% Precision: If there is any doubt about whether two edges are the same, MUST place them in separate clusters.  \n",
    "\n",
    "8. Preserve Original Data: DO NOT introduce new items, create duplicates, or omit any edge from your final output.  \n",
    "\n",
    "9. Output Format: Always return results in valid JSON format. You MUST USE GIVEN KEY. The output must be a list of lists, where each cluster is its own list. The total number of output entries must match the input entries.\n",
    "\n",
    "10. Choose Cluster Representative:\n",
    "  10.1 For every cluster containing more than one edge, YOU MUST choose exactly one representative edge.\n",
    "  10.2 The representative edge must be the most appropriate and easy-to-understand version.\n",
    "  10.3 Enclose the representative edge with '**' (double asterisks) in the output.\n",
    "  10.4 For single-element clusters, no representative modification is required.\n",
    "\n",
    "Read the input list, and return clustered edges, strictly following the given guidelines above.\n",
    "\n",
    "Example input 1:\"21329\":  ['[protein] could interact with [metabolite]', '[protein] could potentially interact with [metabolite]', '[protein] might interact with [metabolite]', '[protein] most definitely interacts with [metabolite]']\n",
    "\n",
    "Example output 1: {\"21329\": [[\"[protein] could interact with [metabolite]\", \"[protein] could potentially interact with [metabolite]\", \"**[protein] might interact with [metabolite]**\"], \"[protein] most difinitely interacts with [metabolite]\"]}\n",
    "\n",
    "Example input 2: \"211\": ['[organism] is beneficial for [process]', '[organism] is beneficial to [organism]', '[organism] beneficial for [process]', '[organism] beneficial for [organism]', '[organism] included plant beneficial microorganisms like [organism]', '[organism] beneficial to [organism]', '[organism] provide beneficial functions to [organism]', '[organism] may be beneficial to [organism]', '[organism] is beneficial for [organism]', '[organism] beneficial to [organ]', '[organism] are beneficial to [organism]', '[organism] confers beneficial traits to [organism]', '[organism] healthy to [organism]', '[organism] useful to [organism]', '[organism] beneficial for [environment]', '[organism] colonization is often of benefit to [organism]', '[interaction] beneficial for [organism]', '[organism] may be beneficial for [organism]', '[organism] provide beneficial services to [organism]']\n",
    "\n",
    "Example output 2: {\"211\": [[\"**[organism] is beneficial for [process]**\", \"[organism] beneficial for [process]\"], [\"**[organism] is beneficial to [organism]**\", \"[organism] beneficial to [organism]\", \"[organism] are beneficial to [organism]\", \"[organism] is beneficial for [organism]\", \"[organism] beneficial for [organism]\"], [\"[organism] included plant beneficial microorganisms like [organism]\"], [\"[organism] provide beneficial functions to [organism]\"], [\"[organism] may be beneficial to [organism]\"], [\"[organism] beneficial to [organ]\"], [\"[organism] confers beneficial traits to [organism]\"], [\"[organism] healthy to [organism]\"], [\"[organism] useful to [organism]\"], [\"[organism] beneficial for [environment]\"], [\"[organism] colonization is often of benefit to [organism]\"], [\"[interaction] beneficial for [organism]\"], [\"[organism] may be beneficial for [organism]\"], [\"[organism] provide beneficial services to [organism]\"]]}\n",
    "\n",
    "Example input 3: \"213\":['[gene] were co-repressed or co-activated by [gene]', '[gene] selective induction/repression of [gene]', '[gene] may be repressed by [gene]', '[gene] could be repressed by [gene]', '[gene] can be repressed by interacting with [gene]', '[gene] are induced or repressed by [gene]', '[gene] repressible by [mutant]', '[gene] were induced or repressed by [other]', '[gene] were induced or repressed by [gene]', '[gene] can be repressed by inducing [gene]', '[gene] are crucial in expressing or repressing [gene]', '[gene] inducing or repressing [gene]', '[gene] induced or repressed by [gene]', '[gene] activated or repressed by [gene]']\n",
    "\n",
    "Example output 3: {\"213\": [[\"[gene] were co-repressed or co-activated by [gene]\"], [\"[gene] selective induction/repression of [gene]\"], [\"**[gene] may be repressed by [gene]**\", \"[gene] could be repressed by [gene]\"], [\"[gene] can be repressed by interacting with [gene]\"], [\"**[gene] are induced or repressed by [gene]**\", \"[gene] were induced or repressed by [gene]\", \"[gene] induced or repressed by [gene]\", \"[gene] inducing or repressing [gene]\", \"[gene] activated or repressed by [gene]\"], [\"[gene] repressible by [mutant]\"], [\"[gene] were induced or repressed by [other]\"], [\"[gene] can be repressed by inducing [gene]\"], [\"[gene] are crucial in expressing or repressing [gene]\"]]}\n",
    "\n",
    "\n",
    "Note: The category enclosed by \"[]\" denotes the node type, and in edge clustering, edges should only be clustered together if both node types match.\n",
    "\n",
    "Warning:\n",
    "Do NOT output newlines or \"\\n\" in the output\n",
    "ALWAYS remember to select a group repressentative enclosed by **, this is critical, and must not be omitted, VERY IMPORTANT\n",
    "BE VERY SURE that you do not add any new terms to the output, and that the output entries EXACTLY matches the input entries, that the output format is correct, and that the output is a list of lists. The total number of output entries must match the input entries.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(array_of_strings):\n",
    "    \"\"\"Prepare GO terms data for embedding.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    input_data = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    for edge in array_of_strings:\n",
    "        text = edge\n",
    "        id = edge.split(\":\")[0]\n",
    "        n_tokens = len(encoding.encode(text))\n",
    "        total_tokens += n_tokens\n",
    "        \n",
    "        input_data.append({\n",
    "            'id': id,\n",
    "            'text': text,\n",
    "            'n_tokens': n_tokens\n",
    "        })\n",
    "\n",
    "    estimated_cost = (total_tokens / 1000000) * 0.1\n",
    "    print(f\"Total number of submissions: {len(input_data)}\")\n",
    "    print(f\"Total input tokens: {total_tokens}\")\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "\n",
    "def create_batch_file(completion_data, query_dir, timestamp, system_prompt, model=\"o3-mini\", model_prefix=None, options=None, chunk_number=None):\n",
    "    \"\"\"Create JSONL file for batch completion requests.\n",
    "    \n",
    "    Args:\n",
    "        completion_data (list): List of dictionaries containing request data\n",
    "        query_dir (str): Directory to save the batch file\n",
    "        timestamp (str): Timestamp for the filename\n",
    "        system_prompt (str): System prompt to use for all requests\n",
    "        model (str): Model to use for completion requests (default: \"o3-mini\")\n",
    "        model_prefix (str): Prefix for the model name\n",
    "        options (dict): Additional model options like temperature or reasoning_effort\n",
    "        chunk_number (optional): Chunk number if processing in batches\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Model: {model}, Model prefix: {model_prefix}, Options: {options}\")\n",
    "    if options is None:\n",
    "        #exit and ask user to provide options\n",
    "        raise ValueError(\"Options are required\")\n",
    "\n",
    "    # Create filename based on model type\n",
    "    if model_prefix is None:\n",
    "        model_prefix = model\n",
    "    batch_fname = f'{model_prefix}_completion_requests_{timestamp}'\n",
    "    if chunk_number is not None:\n",
    "        batch_fname += f'_chunk_{chunk_number}'\n",
    "    batch_fname += '.jsonl'\n",
    "    batch_file_path = os.path.join(query_dir, batch_fname)\n",
    "\n",
    "    with open(batch_file_path, 'w') as f:\n",
    "        for item in completion_data:\n",
    "            request = {\n",
    "                \"custom_id\": item['id'],\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": model,\n",
    "                    \"response_format\": {\"type\": \"json_object\"},\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_prompt\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": item['text']\n",
    "                        }\n",
    "                    ],\n",
    "                    **options\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(request) + '\\n')\n",
    "    \n",
    "    return batch_file_path\n",
    "\n",
    "\n",
    "def submit_batch_job(client, batch_file_path, description):\n",
    "    \"\"\"Submit batch job to OpenAI API.\"\"\"\n",
    "    \n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(batch_file_path, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "    #check if file is uploaded successfully\n",
    "    while batch_input_file.status != \"processed\":\n",
    "        time.sleep(1)\n",
    "    \n",
    "    batch = client.batches.create(\n",
    "        input_file_id=batch_input_file.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"description\": description}\n",
    "    )\n",
    "    print(f\"batch created\\n{batch.id}\")\n",
    "    return batch\n",
    "\n",
    "\n",
    "def create_multiple_batch_files(client, input_array, timestamp, query_dir, system_prompt, model, model_prefix, options, max_batch_size=50_000, description=\"Edge embeddings batch job\"):\n",
    "    \"\"\"Create multiple batch files for embedding.\"\"\"\n",
    "    print(f\"Model: {model}, Model prefix: {model_prefix}, Options: {options}\")\n",
    "    #split embeddings_array into chunks of max_batch_size\n",
    "    chunks = [input_array[i:i+max_batch_size] for i in range(0, len(input_array), max_batch_size)]\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    batch_ids = []\n",
    "    \n",
    "    batch_file_paths = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        batch_file_path = create_batch_file(chunk, query_dir, timestamp, system_prompt, model = model, model_prefix=model_prefix, options=options, chunk_number=i)\n",
    "        batch_file_paths.append(batch_file_path)\n",
    "        batch= submit_batch_job(client, batch_file_path, description)\n",
    "        batch_ids.append(batch.id)\n",
    "        \n",
    "\n",
    "    #save the batch_ids and batch_file_paths to a csv file\n",
    "    batch_info = pd.DataFrame({\"batch_id\": batch_ids, \"batch_file_path\": batch_file_paths})\n",
    "    batch_info.to_csv(os.path.join(query_dir, f\"batch_info_{timestamp}.csv\"), index=False)\n",
    "        \n",
    "    return batch_ids, batch_file_paths\n",
    "\n",
    "\n",
    "def wait_for_completion(client, batch_id, initial_pause=4):\n",
    "    \"\"\"Wait for batch job completion with exponential backoff.\"\"\"\n",
    "    pause = initial_pause\n",
    "    completed = False\n",
    "    while not completed:\n",
    "        batch = client.batches.retrieve(batch_id)\n",
    "        print(batch.status)\n",
    "        \n",
    "        if batch.status == \"completed\":\n",
    "            print(\"Batch completed\")\n",
    "            completed = True\n",
    "            return batch\n",
    "        \n",
    "        print(f\"Batch not completed, pausing for {pause} seconds\")\n",
    "        time.sleep(pause)\n",
    "        pause *= 2\n",
    "\n",
    "\n",
    "def wait_for_completion_for_multiple_batches(client, batch_ids, initial_pause=4):\n",
    "    \"\"\"Wait for completion of multiple batch jobs with exponential backoff.\"\"\"\n",
    "    pause = initial_pause\n",
    "    all_completed = False\n",
    "    #mark all batches as not completed\n",
    "    batch_statuses_dict = {batch_id: \"bla\" for batch_id in batch_ids}\n",
    "    batches = []\n",
    "    while all_completed == False:\n",
    "        for batch_id in batch_ids:\n",
    "            if batch_statuses_dict[batch_id] != \"completed\":\n",
    "                batch = client.batches.retrieve(batch_id)\n",
    "                if batch.status == \"completed\":\n",
    "                    batch_statuses_dict[batch_id] = batch.status\n",
    "                    batches.append(batch)\n",
    "                else:\n",
    "                    batch_statuses_dict[batch_id] = batch.status\n",
    "                \n",
    "        #check if all batches are completed\n",
    "        if all(status == \"completed\" for status in batch_statuses_dict.values()):\n",
    "            print(\"All batches completed\")\n",
    "            print(batch_statuses_dict)\n",
    "            all_completed = True\n",
    "            return batches\n",
    "        \n",
    "        \n",
    "        print(batch_statuses_dict)\n",
    "        \n",
    "        if pause > 300:\n",
    "            pause = 300\n",
    "            \n",
    "        print(f\"Batch not completed, pausing for {pause} seconds\")\n",
    "        time.sleep(pause)\n",
    "        pause *= 2\n",
    "\n",
    "\n",
    "def process_output(client, batch, output_dir, timestamp, batch_file_path, chunk_number=None, model_prefix=\"completion\"):\n",
    "    \"\"\"Download jsonl files from openai and save them to the output_dir\"\"\"\n",
    "\n",
    "    # Save output\n",
    "    output_fname = f'embedding_output_{timestamp}'\n",
    "    if chunk_number is not None:\n",
    "        output_fname = f'{model_prefix}_output_{timestamp}_chunk_{chunk_number}.jsonl'\n",
    "    output_path = os.path.join(output_dir, output_fname)\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"output file already exists, skipping {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "    batch_output_file = client.files.content(batch.output_file_id)\n",
    "    content = batch_output_file.text\n",
    "    split_content = content.split(\"\\n\")[:-1]\n",
    "    \n",
    "    print(f\"batch output file saved to {output_path}\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        for line in split_content:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "    # Save batch info\n",
    "    batch_info = {\n",
    "        \"batch_file_path\": batch_file_path,\n",
    "        \"batch_id\": batch.id,\n",
    "        \"output_file_id\": batch.output_file_id\n",
    "    }\n",
    "    \n",
    "    log_fname = f'{model_prefix}_log_{timestamp}.json'\n",
    "    if chunk_number is not None:\n",
    "        log_fname = f'{model_prefix}_log_{timestamp}_chunk_{chunk_number}.json'\n",
    "    log_path = os.path.join(output_dir, log_fname)\n",
    "    \n",
    "    with open(log_path, 'w') as f:\n",
    "        json.dump(batch_info, f, indent=4)\n",
    "\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "def process_output_for_multiple_batches(client, batch_ids, batch_file_paths, output_dir, timestamp, model_prefix):\n",
    "    \"\"\"Calls process_output for each batch and returns a list of output paths\"\"\"\n",
    "    i = 0\n",
    "    output_paths = []\n",
    "    for batch_id, batch_file_path in zip(batch_ids, batch_file_paths):\n",
    "        batch = client.batches.retrieve(batch_id)\n",
    "        output_path = process_output(client, batch, output_dir, timestamp, batch_file_path, chunk_number=i, model_prefix=model_prefix)\n",
    "\n",
    "        i += 1\n",
    "        output_paths.append(output_path)\n",
    "    \n",
    "    return output_paths\n",
    "\n",
    "\n",
    "def load_output_jsonl(file_path, client=None, api_key=None):\n",
    "    \"\"\"Load output from JSONL file.\"\"\"\n",
    "    with open(file_path, 'rt') as f:  # Changed to gzip.open with text mode\n",
    "        data = []\n",
    "        for line in f:\n",
    "\n",
    "                json_obj = json.loads(line)\n",
    "                embedding = json_obj['response']['body']['choices'][0]['message']['content']\n",
    "                data.append({\n",
    "                    'id': json_obj['custom_id'],\n",
    "                    'embedding': embedding\n",
    "                })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_output_for_multiple_batches(output_paths, client=None, api_key=None):\n",
    "    \"\"\"Calls load_output_jsonl for each output path and concatenates the results.\"\"\"\n",
    "    output_dfs = pd.concat([load_output_jsonl(output_path, client, api_key) for output_path in output_paths])\n",
    "    return output_dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of clusters: 116976\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cluster_df = pd.read_parquet('/home/mads/connectome/data/embeddings/edge_embeddings/clustering/edge_clusters_6_max_clust_30.parquet')\n",
    "print(f\"Total number of clusters: {len(cluster_df)}\")\n",
    "#search the lists in ids_in_cluster for strings containing \"interacts with\"\n",
    "#cluster_df = cluster_df[cluster_df['ids_in_cluster'].apply(lambda x: any(\"interact\" in item.lower() for item in x))]\n",
    "cluster_df = cluster_df[cluster_df.cluster_size > 2]\n",
    "\n",
    "#pick 1000 random rows from cluster_df, set seed to 42\n",
    "cluster_df = cluster_df.sample(n=10, random_state=42)\n",
    "\n",
    "\n",
    "#create the input string. the string should be \"index: the list of edges as a string\"\n",
    "cluster_df[\"input\"] = cluster_df.apply(lambda row: f\"{row.name}: {row['ids_in_cluster']}\", axis=1)\n",
    "inputs = cluster_df[\"input\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key for OpenAI\n",
    "with open(\"data/api_key.txt\", \"r\") as f:\n",
    "    api_key = f.read()\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "query_dir = \"/home/mads/connectome/data/predictions/queries\"\n",
    "# Check if the query directory exists; if not, create it\n",
    "if not os.path.exists(query_dir):\n",
    "    os.makedirs(query_dir)\n",
    "\n",
    "output_dir = \"/home/mads/connectome/data/predictions/output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Prepare embedding data from unique connection types\n",
    "run_from_scratch = input(\"Do you want to run from scratch? (y/n): \")\n",
    "#assert that run_from_scratch is either \"y\" or \"n\", and give a message if it is not\n",
    "assert run_from_scratch in [\"y\", \"n\"], \"run_from_scratch must be either 'y' or 'n'\"\n",
    "batch_size = 5\n",
    "\n",
    "model_prefix = \"v9\"\n",
    "model = \"ft:gpt-4o-mini-2024-07-18:mutwil-lab:4omini-v9-train-1306-test-78:B9u5DUsf\"\n",
    "options = {\"temperature\": 0}\n",
    "description = \"Edge disambiguation\"\n",
    "\n",
    "\n",
    "if run_from_scratch == \"y\":\n",
    "\n",
    "    # Generate a timestamp for file identification\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    print(f\"Timestamp file identifier: {timestamp}\")\n",
    "\n",
    "    input_array = prepare_data(inputs)\n",
    "\n",
    "    batch_ids, batch_file_paths= create_multiple_batch_files(client = client,\n",
    "                                                                                input_array = input_array, \n",
    "                                                                                timestamp = timestamp, \n",
    "                                                                                query_dir = query_dir, \n",
    "                                                                                system_prompt = system_prompt, \n",
    "                                                                                model = model, \n",
    "                                                                                model_prefix = model_prefix,\n",
    "                                                                                options = options,\n",
    "                                                                                max_batch_size = batch_size,\n",
    "                                                                                description = description)\n",
    "    \n",
    "if run_from_scratch == \"n\":\n",
    "    #load the batch_info from the csv file\n",
    "    #get user input for the timestamp\n",
    "    timestamp = input(\"Enter the timestamp for the run you want to load: \")\n",
    "    batch_info = pd.read_csv(os.path.join(query_dir, f\"batch_info_{timestamp}.csv\"))\n",
    "    batch_ids = batch_info[\"batch_id\"].tolist()\n",
    "    batch_file_paths = batch_info[\"batch_file_path\"].tolist()\n",
    "\n",
    "\n",
    "\n",
    "wait_for_completion_for_multiple_batches(client, batch_ids, initial_pause=4)\n",
    "\n",
    "output_paths = process_output_for_multiple_batches(client, batch_ids, batch_file_paths, output_dir, timestamp, model_prefix)\n",
    "\n",
    "output_df = load_output_for_multiple_batches(output_paths, client, api_key)\n",
    "\n",
    "output_df.to_excel(f\"/home/mads/connectome/data/predictions/{model_prefix}_output_{timestamp}.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
