{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read kmeans clusters file\n",
    "df = pd.read_parquet('/path/to/kmeans_clusters/multisect_kmeans_all_clusters_max_clust_30_type_disamb.parquet',engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393892"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove clusters with size equal to 1\n",
    "df = df[df['cluster_size'] > 1]\n",
    "df = df.reset_index(drop=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['node_ids_in_cluster'] = df['node_ids_in_cluster'].apply(lambda x: x.tolist() if hasattr(x, \"tolist\") else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>cluster_size</th>\n",
       "      <th>node_ids_in_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174666</td>\n",
       "      <td>6</td>\n",
       "      <td>[ancestral stress treatment [environment], anc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4934</td>\n",
       "      <td>3</td>\n",
       "      <td>[substitution of Val by Met [mutation], Tyr to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>329097</td>\n",
       "      <td>27</td>\n",
       "      <td>[pairwise alignments [phenotype], Comparative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148225</td>\n",
       "      <td>6</td>\n",
       "      <td>[known orthologs of A. thaliana [organism], co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>281542</td>\n",
       "      <td>23</td>\n",
       "      <td>[Erianthus arundinaceus [organism], E. sativa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393887</th>\n",
       "      <td>318199</td>\n",
       "      <td>6</td>\n",
       "      <td>[PIF6-beta [rna], loss of EIN4 [gene], Loss of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393888</th>\n",
       "      <td>449343</td>\n",
       "      <td>10</td>\n",
       "      <td>[MADS-box TFs [gene], TFs from the MADS-box pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393889</th>\n",
       "      <td>160525</td>\n",
       "      <td>3</td>\n",
       "      <td>[the motor domain with ATPase activity [protei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393890</th>\n",
       "      <td>179048</td>\n",
       "      <td>2</td>\n",
       "      <td>[wider repla [phenotype], larger sizes of reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393891</th>\n",
       "      <td>147850</td>\n",
       "      <td>8</td>\n",
       "      <td>[seedling transcriptomes [tissue], Whole trans...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393892 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cluster_id  cluster_size  \\\n",
       "0           174666             6   \n",
       "1             4934             3   \n",
       "2           329097            27   \n",
       "3           148225             6   \n",
       "4           281542            23   \n",
       "...            ...           ...   \n",
       "393887      318199             6   \n",
       "393888      449343            10   \n",
       "393889      160525             3   \n",
       "393890      179048             2   \n",
       "393891      147850             8   \n",
       "\n",
       "                                      node_ids_in_cluster  \n",
       "0       [ancestral stress treatment [environment], anc...  \n",
       "1       [substitution of Val by Met [mutation], Tyr to...  \n",
       "2       [pairwise alignments [phenotype], Comparative ...  \n",
       "3       [known orthologs of A. thaliana [organism], co...  \n",
       "4       [Erianthus arundinaceus [organism], E. sativa ...  \n",
       "...                                                   ...  \n",
       "393887  [PIF6-beta [rna], loss of EIN4 [gene], Loss of...  \n",
       "393888  [MADS-box TFs [gene], TFs from the MADS-box pr...  \n",
       "393889  [the motor domain with ATPase activity [protei...  \n",
       "393890  [wider repla [phenotype], larger sizes of reco...  \n",
       "393891  [seedling transcriptomes [tissue], Whole trans...  \n",
       "\n",
       "[393892 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#randomize rows with a seed\n",
    "seed = 42\n",
    "df_randomized_rows = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "df_randomized_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create df_sample each with 50k rows from df_randomized_rows due to api limitations\n",
    "df_sample_0_50k = df_randomized_rows.iloc[0:50000]  # 0-50k\n",
    "df_sample_50k_100k = df_randomized_rows.iloc[50000:100000] # 50k-100k\n",
    "df_sample_100k_150k = df_randomized_rows.iloc[100000:150000] # 100k-150k\n",
    "df_sample_150k_200k = df_randomized_rows.iloc[150000:200000] # 150k-200k\n",
    "df_sample_200k_250k = df_randomized_rows.iloc[200000:250000] # 200k-250k\n",
    "df_sample_250k_300k = df_randomized_rows.iloc[250000:300000] # 250k-300k\n",
    "df_sample_300k_350k = df_randomized_rows.iloc[300000:350000] # 300k-350k\n",
    "df_sample_350k_393k = df_randomized_rows.iloc[350000:] # 350k-393k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000, 50000, 50000, 50000, 50000, 50000, 43892)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sample_0_50k), len(df_sample_50k_100k), len(df_sample_100k_150k), len(df_sample_150k_200k), len(df_sample_200k_250k), len(df_sample_250k_300k), len(df_sample_300k_350k), len(df_sample_350k_393k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create corresponding entity list for each df_sample\n",
    "\n",
    "entity_list_0_50k = df_sample_0_50k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()\n",
    "entity_list_50k_100k = df_sample_50k_100k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()\n",
    "entity_list_100k_150k = df_sample_100k_150k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()\n",
    "entity_list_150k_200k = df_sample_150k_200k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()\n",
    "entity_list_200k_250k = df_sample_200k_250k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()\n",
    "entity_list_250k_300k = df_sample_250k_300k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()\n",
    "entity_list_300k_350k = df_sample_300k_350k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()\n",
    "entity_list_350k_393k = df_sample_350k_393k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000, 50000, 50000, 50000, 50000, 50000, 43892)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_list_0_50k), len(entity_list_50k_100k), len(entity_list_100k_150k), len(entity_list_150k_200k), len(entity_list_200k_250k), len(entity_list_250k_300k), len(entity_list_300k_350k), len(entity_list_350k_393k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4omini after finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_message = \"\"\"\\n\n",
    "You are a data scientist specializing in grouping plant biological entities. Your task is to cluster similar entities while strictly adhering to the following guidelines:\\n\\t1.\\tExact Phrase Matching Matters: \\n1.1 Consider the Entire Phrase: Treat each entity as a single, whole phrase. This includes all key biological terms and any bracketed text\\n1.2 Ignore Minor Surface Differences: Minor variations such as letter casing (uppercase vs. lowercase), spacing, punctuation, standard abbreviations, or singular vs. plural forms do not create new or separate entities.\\n\\t2.\\tStrict (100%) Key Term Separation: If an entity has a different key biological term, it MUST GO into a separate cluster.\\n3. Sub-identifier separation: If an entity differs by any numeric value, sub-identifier, or qualifier, they MUST BE placed in separate clusters.\\n\\t4.\\tAvoid False Similarity: DO NOT cluster two entities together simply because they share a common word or term if their overall key term or concept is different.\\n5. Extra Descriptor Differentiation: If one entity has an extra descriptor that changes its meaning, do not group them together.\\n\\t6.\\tStrict Synonym/Near-Synonym Grouping: Only group entities together if they refer to the exact same biological structure, process, or concept.\\n\\t7.\\tMaintain 100% Precision: If there is any doubt about whether two entities are the same, MUST place them in separate clusters.\\n\\t8.\\tPreserve Original Data: DO NOT introduce new items, create duplicates, or omit any entity from your final output.\\n\\t9.\\tOutput Format: Always return results in valid JSON format. You MUST USE GIVEN KEY.\\n10. Choose cluster representative: YOU MUST pickup most appropriate and easy-to-understand cluster representative and enclose it with '**', if there is more than one entity in that particular cluster. For example, pick the full term instead of an abbreviation.\\n\\nRead the input list, and return clustered entities, STRICTLY following the given guidelines above.\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def split_dict(d, chunk_size=100):\n",
    "    keys = list(d.keys())\n",
    "    chunks = []\n",
    "    for i in range(0, len(keys), chunk_size):\n",
    "        chunk_keys = keys[i:i+chunk_size]\n",
    "        chunk_dict = {k: d[k] for k in chunk_keys}\n",
    "        chunks.append(chunk_dict)\n",
    "    return chunks\n",
    "\n",
    "def flatten_bracketed_strings(value_list):\n",
    "    \"\"\"\n",
    "    Takes a list. For each item:\n",
    "      - If item is a string that looks like '[...]', parse it and extend the list.\n",
    "      - Otherwise, keep as is.\n",
    "    Returns a new flattened list.\n",
    "    \"\"\"\n",
    "    new_list = []\n",
    "    for val in value_list:\n",
    "        if (\n",
    "            isinstance(val, str) \n",
    "            and val.strip().startswith(\"[\") \n",
    "            and val.strip().endswith(\"]\")\n",
    "        ):\n",
    "            # Attempt to parse the bracketed string\n",
    "            try:\n",
    "                parsed = ast.literal_eval(val)  # convert string -> Python list\n",
    "                if isinstance(parsed, list):\n",
    "                    new_list.extend(parsed)  # flatten\n",
    "                else:\n",
    "                    # If it's not a list, just append as-is\n",
    "                    new_list.append(val)\n",
    "            except (SyntaxError, ValueError):\n",
    "                # If parsing fails, keep original\n",
    "                new_list.append(val)\n",
    "        else:\n",
    "            new_list.append(val)\n",
    "    return new_list\n",
    "\n",
    "def prepend_key_to_values(d):\n",
    "    \"\"\"\n",
    "    For each key k in d:\n",
    "      1) Ensure value is a *list*.\n",
    "      2) Flatten bracketed strings if needed.\n",
    "      3) Prepend k to the final list.\n",
    "    \"\"\"\n",
    "    for k in d:\n",
    "        val = d[k]\n",
    "        # 1) If not a list, make it one\n",
    "        if not isinstance(val, list):\n",
    "            val = [val]\n",
    "\n",
    "        # 2) Flatten bracketed strings\n",
    "        val = flatten_bracketed_strings(val)\n",
    "\n",
    "        # 3) Prepend key\n",
    "        #d[k] = [k] + val\n",
    "    return d\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "#chunks = split_dict(entity_list, 1)\n",
    "#print(len(chunks))  # Should be 3 if entity_list has 953 keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create chunks for each entity dict\n",
    "\n",
    "chunks_0_50k = split_dict(entity_list_0_50k, 1)\n",
    "chunks_50k_100k = split_dict(entity_list_50k_100k, 1)\n",
    "chunks_100k_150k = split_dict(entity_list_100k_150k, 1)\n",
    "chunks_150k_200k = split_dict(entity_list_150k_200k, 1)\n",
    "chunks_200k_250k = split_dict(entity_list_200k_250k, 1)\n",
    "chunks_250k_300k = split_dict(entity_list_250k_300k, 1)\n",
    "chunks_300k_350k = split_dict(entity_list_300k_350k, 1)\n",
    "chunks_350k_393k = split_dict(entity_list_350k_393k, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000, 50000, 50000, 50000, 50000, 50000, 43892)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_0_50k), len(chunks_50k_100k), len(chunks_100k_150k), len(chunks_150k_200k), len(chunks_200k_250k), len(chunks_250k_300k), len(chunks_300k_350k), len(chunks_350k_393k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "\n",
    "output_path = 'path/to/finetuned_model/outputs/'\n",
    "\n",
    "# Define a dictionary of files to write\n",
    "# key = output filename suffix, value = the list of chunks\n",
    "chunk_groups = {\n",
    "    '0_50k': chunks_0_50k,\n",
    "    '50k_100k': chunks_50k_100k,\n",
    "    '100k_150k': chunks_100k_150k,\n",
    "    '150k_200k': chunks_150k_200k,\n",
    "    '200k_250k': chunks_200k_250k,\n",
    "    '250k_300k': chunks_250k_300k,\n",
    "    '300k_350k': chunks_300k_350k,\n",
    "    '350k_393k': chunks_350k_393k\n",
    "}\n",
    "\n",
    "for suffix, chunk_list in chunk_groups.items():\n",
    "    out_file = f'v8_4omini_maxclust30_{suffix}.jsonl'\n",
    "    with jsonlines.open(output_path + out_file, mode='w') as file:\n",
    "        for i, chunk in enumerate(chunk_list):\n",
    "            line = {\n",
    "                \"custom_id\": str(i),\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": \"ft:gpt-4o-mini-2024-07-18:mutwil-lab:4omini-v8-train-1306-test-78:B9DybEeH\", #name of the finetuned model\n",
    "                    \"temperature\": 0,\n",
    "                    \"top_p\": 0,\n",
    "                    \"frequency_penalty\": 0,\n",
    "                    \"presence_penalty\": 0,\n",
    "                    \"response_format\": {\"type\": \"json_object\"},\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_message\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": json.dumps(chunk, separators=(',', ':'))\n",
    "                        }\n",
    "                    ],\n",
    "                    \"max_tokens\": 16384\n",
    "                }\n",
    "            }\n",
    "            file.write(line)\n",
    "    print(f'Done writing to {output_path + out_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of all files created\n",
    "files_created = [output_path + f'v8_4omini_maxclust30_{suffix}.jsonl' for suffix in chunk_groups.keys()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upload to batchapi server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create batchapi upload\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI Client\n",
    "client = OpenAI(api_key=\"Enter your OpenAI API key here\")\n",
    "\n",
    "for fname in files_created:\n",
    "    # Upload the file first\n",
    "    file_upload = client.files.create(\n",
    "        file=open(fname, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    print(f\"Uploaded file id: {file_upload.id}\")\n",
    "\n",
    "    # Create the batch job referencing the uploaded file ID\n",
    "    batch_response = client.batches.create(\n",
    "        input_file_id=file_upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\"\n",
    "    )\n",
    "\n",
    "    print(f\"Batch job created: {batch_response.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert jsonl output files to Csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using os list dir get all the batch api outputs from batchapi_final_inps\n",
    "\n",
    "import os\n",
    "import jsonlines\n",
    "import json\n",
    "\n",
    "# Define the path to the directory containing the batch API output files\n",
    "batch_output_path = 'path/to/your/batchapi_final_outs/'\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "batch_output_files = os.listdir(batch_output_path)\n",
    "\n",
    "\n",
    "#only get the jsonl files\n",
    "batch_output_files = [f for f in batch_output_files if f.endswith('.jsonl')]\n",
    "\n",
    "# Sort the files to ensure they are processed in order\n",
    "batch_output_files.sort()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import csv\n",
    "#now read each jsonl file and convert to csv\n",
    "# Define the path to the directory where the CSV files will be saved\n",
    "output_csv_path = '/path/to/your/batchapi_final_outs/'\n",
    "csv_file_names = ['4omini_v8_0_50k.csv', '4omini_v8_50k_100k.csv', '4omini_v8_100k_150k.csv', '4omini_v8_150k_200k.csv', '4omini_v8_200k_250k.csv', '4omini_v8_250k_300k.csv', '4omini_v8_300k_350k.csv', '4omini_v8_350k_393k.csv']\n",
    "\n",
    "for i, batch_file in enumerate(batch_output_files):\n",
    "    skip_lines = 0\n",
    "    # Open the JSONL file\n",
    "    with jsonlines.open(batch_output_path + batch_file) as reader:\n",
    "        # Define the CSV file name\n",
    "        csv_file = csv_file_names[i]\n",
    "        # Open the CSV file for writing\n",
    "        with open(output_csv_path + csv_file, 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # Write the header row\n",
    "            writer.writerow([\"Cluster Id\", \"Group Items\"])\n",
    "            for line in reader:\n",
    "                try:\n",
    "                    body = line[\"response\"][\"body\"]\n",
    "                    # Get the JSON string from the assistant's \"content\"\n",
    "                    content_str = body[\"choices\"][0][\"message\"][\"content\"]\n",
    "                    data = json.loads(content_str)\n",
    "                    #print(len(data.keys()))\n",
    "                    all_keys = list(data.keys())\n",
    "                    all_values = list(data.values())\n",
    "\n",
    "                    for i in range(len(all_keys)):\n",
    "                        #writer.writerow([all_keys[i], all_values[i]])\n",
    "                        writer.writerow([all_keys[i], json.dumps(all_values[i], ensure_ascii=False)])\n",
    "                \n",
    "                    \n",
    "                    \n",
    "\n",
    "                except (KeyError, IndexError, json.JSONDecodeError):\n",
    "                   # Handle the case where the expected keys are not present\n",
    "                    skip_lines += 1\n",
    "            \n",
    "                    \n",
    "                    pass\n",
    "    print(f\"Skipped {skip_lines} lines in {batch_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connectome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
