{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read kmeans clusters\n",
    "df = pd.read_parquet('/Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/multisect_kmeans_all_clusters_max_clust_30_type_disamb_missing.parquet',engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "893"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove clusters with size equal to 1\n",
    "df = df[df['cluster_size'] > 1]\n",
    "df = df.reset_index(drop=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['node_ids_in_cluster'] = df['node_ids_in_cluster'].apply(lambda x: x.tolist() if hasattr(x, \"tolist\") else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>cluster_size</th>\n",
       "      <th>node_ids_in_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>174666</td>\n",
       "      <td>6</td>\n",
       "      <td>[ancestral stress treatment [environment], anc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4934</td>\n",
       "      <td>3</td>\n",
       "      <td>[substitution of Val by Met [mutation], Tyr to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>329097</td>\n",
       "      <td>27</td>\n",
       "      <td>[pairwise alignments [phenotype], Comparative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148225</td>\n",
       "      <td>6</td>\n",
       "      <td>[known orthologs of A. thaliana [organism], co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>281542</td>\n",
       "      <td>23</td>\n",
       "      <td>[Erianthus arundinaceus [organism], E. sativa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393887</th>\n",
       "      <td>318199</td>\n",
       "      <td>6</td>\n",
       "      <td>[PIF6-beta [rna], loss of EIN4 [gene], Loss of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393888</th>\n",
       "      <td>449343</td>\n",
       "      <td>10</td>\n",
       "      <td>[MADS-box TFs [gene], TFs from the MADS-box pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393889</th>\n",
       "      <td>160525</td>\n",
       "      <td>3</td>\n",
       "      <td>[the motor domain with ATPase activity [protei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393890</th>\n",
       "      <td>179048</td>\n",
       "      <td>2</td>\n",
       "      <td>[wider repla [phenotype], larger sizes of reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393891</th>\n",
       "      <td>147850</td>\n",
       "      <td>8</td>\n",
       "      <td>[seedling transcriptomes [tissue], Whole trans...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393892 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cluster_id  cluster_size  \\\n",
       "0           174666             6   \n",
       "1             4934             3   \n",
       "2           329097            27   \n",
       "3           148225             6   \n",
       "4           281542            23   \n",
       "...            ...           ...   \n",
       "393887      318199             6   \n",
       "393888      449343            10   \n",
       "393889      160525             3   \n",
       "393890      179048             2   \n",
       "393891      147850             8   \n",
       "\n",
       "                                      node_ids_in_cluster  \n",
       "0       [ancestral stress treatment [environment], anc...  \n",
       "1       [substitution of Val by Met [mutation], Tyr to...  \n",
       "2       [pairwise alignments [phenotype], Comparative ...  \n",
       "3       [known orthologs of A. thaliana [organism], co...  \n",
       "4       [Erianthus arundinaceus [organism], E. sativa ...  \n",
       "...                                                   ...  \n",
       "393887  [PIF6-beta [rna], loss of EIN4 [gene], Loss of...  \n",
       "393888  [MADS-box TFs [gene], TFs from the MADS-box pr...  \n",
       "393889  [the motor domain with ATPase activity [protei...  \n",
       "393890  [wider repla [phenotype], larger sizes of reco...  \n",
       "393891  [seedling transcriptomes [tissue], Whole trans...  \n",
       "\n",
       "[393892 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#randomize rows with a seed\n",
    "seed = 42\n",
    "df_randomized_rows = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "df_randomized_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create df_sample each with 50k rows from df_randomized_rows\n",
    "df_sample_0_50k = df_randomized_rows.iloc[0:50000]  # 0-50k\n",
    "df_sample_50k_100k = df_randomized_rows.iloc[50000:100000] # 50k-100k\n",
    "df_sample_100k_150k = df_randomized_rows.iloc[100000:150000] # 100k-150k\n",
    "df_sample_150k_200k = df_randomized_rows.iloc[150000:200000] # 150k-200k\n",
    "df_sample_200k_250k = df_randomized_rows.iloc[200000:250000] # 200k-250k\n",
    "df_sample_250k_300k = df_randomized_rows.iloc[250000:300000] # 250k-300k\n",
    "df_sample_300k_350k = df_randomized_rows.iloc[300000:350000] # 300k-350k\n",
    "df_sample_350k_393k = df_randomized_rows.iloc[350000:] # 350k-393k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000, 50000, 50000, 50000, 50000, 50000, 43892)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sample_0_50k), len(df_sample_50k_100k), len(df_sample_100k_150k), len(df_sample_150k_200k), len(df_sample_200k_250k), len(df_sample_250k_300k), len(df_sample_300k_350k), len(df_sample_350k_393k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create corresponding entity list for each df_sample\n",
    "\n",
    "entity_list_0_50k = df_sample_0_50k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()\n",
    "entity_list_50k_100k = df_sample_50k_100k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()\n",
    "entity_list_100k_150k = df_sample_100k_150k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()\n",
    "entity_list_150k_200k = df_sample_150k_200k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()\n",
    "entity_list_200k_250k = df_sample_200k_250k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()\n",
    "entity_list_250k_300k = df_sample_250k_300k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()\n",
    "entity_list_300k_350k = df_sample_300k_350k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()\n",
    "entity_list_350k_393k = df_sample_350k_393k.set_index('cluster_id')['node_ids_in_cluster'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000, 50000, 50000, 50000, 50000, 50000, 43892)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_list_0_50k), len(entity_list_50k_100k), len(entity_list_100k_150k), len(entity_list_150k_200k), len(entity_list_200k_250k), len(entity_list_250k_300k), len(entity_list_300k_350k), len(entity_list_350k_393k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4omini after finetuning V9 prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_message = \"\"\"\\n\n",
    "You are a data scientist specializing in grouping plant biological entities. Your task is to cluster similar entities while strictly adhering to the following guidelines:\\n\\t1.\\tExact Phrase Matching Matters: \\n1.1 Consider the Entire Phrase: Treat each entity as a single, whole phrase. This includes all key biological terms and any bracketed text\\n1.2 Ignore Minor Surface Differences: Minor variations such as letter casing (uppercase vs. lowercase), spacing, punctuation, standard abbreviations, or singular vs. plural forms do not create new or separate entities.\\n\\t2.\\tStrict (100%) Key Term Separation: If an entity has a different key biological term, it MUST GO into a separate cluster.\\n3. Sub-identifier separation: If an entity differs by any numeric value, sub-identifier, or qualifier, they MUST BE placed in separate clusters.\\n\\t4.\\tAvoid False Similarity: DO NOT cluster two entities together simply because they share a common word or term if their overall key term or concept is different.\\n5. Extra Descriptor Differentiation: If one entity has an extra descriptor that changes its meaning, do not group them together.\\n\\t6.\\tStrict Synonym/Near-Synonym Grouping: Only group entities together if they refer to the exact same biological structure, process, or concept.\\n\\t7.\\tMaintain 100% Precision: If there is any doubt about whether two entities are the same, MUST place them in separate clusters.\\n\\t8.\\tPreserve Original Data: DO NOT introduce new items, create duplicates, or omit any entity from your final output.\\n\\t9.\\tOutput Format: Always return results in valid JSON format. You MUST USE GIVEN KEY.\\n10. Choose cluster representative: YOU MUST pickup most appropriate and easy-to-understand cluster representative and enclose it with '**', if there is more than one entity in that particular cluster. For example, pick the full term instead of an abbreviation.\\n\\nRead the input list, and return clustered entities, STRICTLY following the given guidelines above.\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def split_dict(d, chunk_size=100):\n",
    "    keys = list(d.keys())\n",
    "    chunks = []\n",
    "    for i in range(0, len(keys), chunk_size):\n",
    "        chunk_keys = keys[i:i+chunk_size]\n",
    "        chunk_dict = {k: d[k] for k in chunk_keys}\n",
    "        chunks.append(chunk_dict)\n",
    "    return chunks\n",
    "\n",
    "def flatten_bracketed_strings(value_list):\n",
    "    \"\"\"\n",
    "    Takes a list. For each item:\n",
    "      - If item is a string that looks like '[...]', parse it and extend the list.\n",
    "      - Otherwise, keep as is.\n",
    "    Returns a new flattened list.\n",
    "    \"\"\"\n",
    "    new_list = []\n",
    "    for val in value_list:\n",
    "        if (\n",
    "            isinstance(val, str) \n",
    "            and val.strip().startswith(\"[\") \n",
    "            and val.strip().endswith(\"]\")\n",
    "        ):\n",
    "            # Attempt to parse the bracketed string\n",
    "            try:\n",
    "                parsed = ast.literal_eval(val)  # convert string -> Python list\n",
    "                if isinstance(parsed, list):\n",
    "                    new_list.extend(parsed)  # flatten\n",
    "                else:\n",
    "                    # If it's not a list, just append as-is\n",
    "                    new_list.append(val)\n",
    "            except (SyntaxError, ValueError):\n",
    "                # If parsing fails, keep original\n",
    "                new_list.append(val)\n",
    "        else:\n",
    "            new_list.append(val)\n",
    "    return new_list\n",
    "\n",
    "def prepend_key_to_values(d):\n",
    "    \"\"\"\n",
    "    For each key k in d:\n",
    "      1) Ensure value is a *list*.\n",
    "      2) Flatten bracketed strings if needed.\n",
    "      3) Prepend k to the final list.\n",
    "    \"\"\"\n",
    "    for k in d:\n",
    "        val = d[k]\n",
    "        # 1) If not a list, make it one\n",
    "        if not isinstance(val, list):\n",
    "            val = [val]\n",
    "\n",
    "        # 2) Flatten bracketed strings\n",
    "        val = flatten_bracketed_strings(val)\n",
    "\n",
    "        # 3) Prepend key\n",
    "        #d[k] = [k] + val\n",
    "    return d\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "#chunks = split_dict(entity_list, 1)\n",
    "#print(len(chunks))  # Should be 3 if entity_list has 953 keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create chunks for each entity dict\n",
    "\n",
    "chunks_0_50k = split_dict(entity_list_0_50k, 1)\n",
    "chunks_50k_100k = split_dict(entity_list_50k_100k, 1)\n",
    "chunks_100k_150k = split_dict(entity_list_100k_150k, 1)\n",
    "chunks_150k_200k = split_dict(entity_list_150k_200k, 1)\n",
    "chunks_200k_250k = split_dict(entity_list_200k_250k, 1)\n",
    "chunks_250k_300k = split_dict(entity_list_250k_300k, 1)\n",
    "chunks_300k_350k = split_dict(entity_list_300k_350k, 1)\n",
    "chunks_350k_393k = split_dict(entity_list_350k_393k, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000, 50000, 50000, 50000, 50000, 50000, 43892)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_0_50k), len(chunks_50k_100k), len(chunks_100k_150k), len(chunks_150k_200k), len(chunks_200k_250k), len(chunks_250k_300k), len(chunks_300k_350k), len(chunks_350k_393k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing to /Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_0_50k.jsonl\n",
      "Done writing to /Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_50k_100k.jsonl\n",
      "Done writing to /Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_100k_150k.jsonl\n",
      "Done writing to /Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_150k_200k.jsonl\n",
      "Done writing to /Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_200k_250k.jsonl\n",
      "Done writing to /Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_250k_300k.jsonl\n",
      "Done writing to /Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_300k_350k.jsonl\n",
      "Done writing to /Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_350k_393k.jsonl\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "\n",
    "output_path = '/Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/'\n",
    "\n",
    "# Define a dictionary of files to write\n",
    "# key = output filename suffix, value = the list of chunks\n",
    "chunk_groups = {\n",
    "    '0_50k': chunks_0_50k,\n",
    "    '50k_100k': chunks_50k_100k,\n",
    "    '100k_150k': chunks_100k_150k,\n",
    "    '150k_200k': chunks_150k_200k,\n",
    "    '200k_250k': chunks_200k_250k,\n",
    "    '250k_300k': chunks_250k_300k,\n",
    "    '300k_350k': chunks_300k_350k,\n",
    "    '350k_393k': chunks_350k_393k\n",
    "}\n",
    "\n",
    "for suffix, chunk_list in chunk_groups.items():\n",
    "    out_file = f'v9_4omini_maxclust30_{suffix}.jsonl'\n",
    "    with jsonlines.open(output_path + out_file, mode='w') as file:\n",
    "        for i, chunk in enumerate(chunk_list):\n",
    "            line = {\n",
    "                \"custom_id\": str(i),\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": \"ft:gpt-4o-mini-2024-07-18:mutwil-lab:4omini-v9-train-1306-test-78:B9u5DUsf\",\n",
    "                    \"temperature\": 0,\n",
    "                    \"top_p\": 0,\n",
    "                    \"frequency_penalty\": 0,\n",
    "                    \"presence_penalty\": 0,\n",
    "                    \"response_format\": {\"type\": \"json_object\"},\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_message\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": json.dumps(chunk, separators=(',', ':'))\n",
    "                        }\n",
    "                    ],\n",
    "                    \"max_tokens\": 16384\n",
    "                }\n",
    "            }\n",
    "            file.write(line)\n",
    "    print(f'Done writing to {output_path + out_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing to /Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_missing.jsonl\n"
     ]
    }
   ],
   "source": [
    "#FOR MISSING ONES\n",
    "\n",
    "\n",
    "import jsonlines\n",
    "import json\n",
    "\n",
    "output_path = '/Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/'\n",
    "\n",
    "# Define a dictionary of files to write\n",
    "# key = output filename suffix, value = the list of chunks\n",
    "chunk_groups = {\n",
    "    'missing': chunks\n",
    "}\n",
    "\n",
    "for suffix, chunk_list in chunk_groups.items():\n",
    "    out_file = f'v9_4omini_maxclust30_{suffix}.jsonl'\n",
    "    with jsonlines.open(output_path + out_file, mode='w') as file:\n",
    "        for i, chunk in enumerate(chunk_list):\n",
    "            line = {\n",
    "                \"custom_id\": str(i),\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": \"ft:gpt-4o-mini-2024-07-18:mutwil-lab:4omini-v9-train-1306-test-78:B9u5DUsf\",\n",
    "                    \"temperature\": 0,\n",
    "                    \"top_p\": 0,\n",
    "                    \"frequency_penalty\": 0,\n",
    "                    \"presence_penalty\": 0,\n",
    "                    \"response_format\": {\"type\": \"json_object\"},\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_message\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": json.dumps(chunk, separators=(',', ':'))\n",
    "                        }\n",
    "                    ],\n",
    "                    \"max_tokens\": 16384\n",
    "                }\n",
    "            }\n",
    "            file.write(line)\n",
    "    print(f'Done writing to {output_path + out_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_0_50k.jsonl',\n",
       " '/Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_50k_100k.jsonl',\n",
       " '/Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_100k_150k.jsonl',\n",
       " '/Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_150k_200k.jsonl',\n",
       " '/Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_200k_250k.jsonl',\n",
       " '/Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_250k_300k.jsonl',\n",
       " '/Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_300k_350k.jsonl',\n",
       " '/Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_350k_393k.jsonl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of all files created\n",
    "files_created = [output_path + f'v9_4omini_maxclust30_{suffix}.jsonl' for suffix in chunk_groups.keys()]\n",
    "files_created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upload to batchapi server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file id: file-SCAn12pCgCCQtPBJoDbFWm\n",
      "Batch job created: batch_67dad4f7587881908da646b1343bf90c\n",
      "Uploaded file id: file-4NKhRAWsTyWsiJJknNMr3h\n",
      "Batch job created: batch_67dad50ad3dc819094ad389c08b222b7\n",
      "Uploaded file id: file-Vf2pq1H5ctGkJ8nEjMFH1T\n",
      "Batch job created: batch_67dad51e19388190b329427c3e872424\n",
      "Uploaded file id: file-YG2dBrcXnqHsXfZq4WyoR4\n",
      "Batch job created: batch_67dad52f57108190bca049af78361d9d\n",
      "Uploaded file id: file-KN9kGhFUaLm9Gadi89dmk2\n",
      "Batch job created: batch_67dad542102081908506fa5a55f4621b\n",
      "Uploaded file id: file-WWb2ru2fzRyDd6Dh7whvnG\n",
      "Batch job created: batch_67dad5530bd88190a7e231c56398e20f\n",
      "Uploaded file id: file-U5GjQprAT4C7cz8HWcBvne\n",
      "Batch job created: batch_67dad563f2548190845a4ca8e6ef1484\n",
      "Uploaded file id: file-JRYYDujDTsnFcfYsWaKzFG\n",
      "Batch job created: batch_67dad5734b0881909d08699386b2662b\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create batchapi upload\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI Client\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "for fname in files_created:\n",
    "    # Upload the file first\n",
    "    file_upload = client.files.create(\n",
    "        file=open(fname, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    print(f\"Uploaded file id: {file_upload.id}\")\n",
    "\n",
    "    # Create the batch job referencing the uploaded file ID\n",
    "    batch_response = client.batches.create(\n",
    "        input_file_id=file_upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\"\n",
    "    )\n",
    "\n",
    "    print(f\"Batch job created: {batch_response.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file id: file-GuSebdYkaft1rojvUCZMVo\n",
      "Batch job created: batch_67dc3affb65481909cc1ca0afa430525\n"
     ]
    }
   ],
   "source": [
    "# SUBMIT MISSING batch\n",
    "\n",
    "\n",
    "#Create batchapi upload\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI Client\n",
    "client = OpenAI(api_key=\"ENTER YOUR API KEY\")\n",
    "fname = '/Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_inps/v9_4omini_maxclust30_missing.jsonl'\n",
    "\n",
    "# Upload the file first\n",
    "file_upload = client.files.create(\n",
    "    file=open(fname, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "print(f\"Uploaded file id: {file_upload.id}\")\n",
    "\n",
    "# Create the batch job referencing the uploaded file ID\n",
    "batch_response = client.batches.create(\n",
    "    input_file_id=file_upload.id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\"\n",
    ")\n",
    "\n",
    "print(f\"Batch job created: {batch_response.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert jsonl output files to Csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['batch_67dad4f7587881908da646b1343bf90c_output.jsonl',\n",
       " 'batch_67dad50ad3dc819094ad389c08b222b7_output.jsonl',\n",
       " 'batch_67dad51e19388190b329427c3e872424_output.jsonl',\n",
       " 'batch_67dad52f57108190bca049af78361d9d_output.jsonl',\n",
       " 'batch_67dad542102081908506fa5a55f4621b_output.jsonl',\n",
       " 'batch_67dad5530bd88190a7e231c56398e20f_output.jsonl',\n",
       " 'batch_67dad563f2548190845a4ca8e6ef1484_output.jsonl',\n",
       " 'batch_67dad5734b0881909d08699386b2662b_output.jsonl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using os list dir get all the batch api outputs from batchapi_final_inps\n",
    "\n",
    "import os\n",
    "import jsonlines\n",
    "import json\n",
    "\n",
    "# Define the path to the directory containing the batch API output files\n",
    "batch_output_path = '/Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_outs/'\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "batch_output_files = os.listdir(batch_output_path)\n",
    "\n",
    "\n",
    "#only get the jsonl files\n",
    "batch_output_files = [f for f in batch_output_files if f.endswith('.jsonl')]\n",
    "\n",
    "# Sort the files to ensure they are processed in order\n",
    "batch_output_files.sort()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 99 lines in batch_67dad4f7587881908da646b1343bf90c_output.jsonl\n",
      "Skipped 99 lines in batch_67dad50ad3dc819094ad389c08b222b7_output.jsonl\n",
      "Skipped 114 lines in batch_67dad51e19388190b329427c3e872424_output.jsonl\n",
      "Skipped 82 lines in batch_67dad52f57108190bca049af78361d9d_output.jsonl\n",
      "Skipped 94 lines in batch_67dad542102081908506fa5a55f4621b_output.jsonl\n",
      "Skipped 87 lines in batch_67dad5530bd88190a7e231c56398e20f_output.jsonl\n",
      "Skipped 91 lines in batch_67dad563f2548190845a4ca8e6ef1484_output.jsonl\n",
      "Skipped 88 lines in batch_67dad5734b0881909d08699386b2662b_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import csv\n",
    "#now read each jsonl file and convert to csv\n",
    "# Define the path to the directory where the CSV files will be saved\n",
    "output_csv_path = '/Users/manojitharajula/Documents/PhD/Connectome/Entity_disambiguation/kmeans/4omini_after_finetuning/batchapi_final_outs/'\n",
    "csv_file_names = ['4omini_v9_0_50k.csv', '4omini_v9_50k_100k.csv', '4omini_v9_100k_150k.csv', '4omini_v9_150k_200k.csv', '4omini_v9_200k_250k.csv', '4omini_v9_250k_300k.csv', '4omini_v9_300k_350k.csv', '4omini_v9_350k_393k.csv']\n",
    "\n",
    "for i, batch_file in enumerate(batch_output_files):\n",
    "    skip_lines = 0\n",
    "    # Open the JSONL file\n",
    "    with jsonlines.open(batch_output_path + batch_file) as reader:\n",
    "        # Define the CSV file name\n",
    "        csv_file = csv_file_names[i]\n",
    "        # Open the CSV file for writing\n",
    "        with open(output_csv_path + csv_file, 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # Write the header row\n",
    "            writer.writerow([\"Cluster Id\", \"Group Items\"])\n",
    "            for line in reader:\n",
    "                try:\n",
    "                    body = line[\"response\"][\"body\"]\n",
    "                    # Get the JSON string from the assistant's \"content\"\n",
    "                    content_str = body[\"choices\"][0][\"message\"][\"content\"]\n",
    "                    data = json.loads(content_str)\n",
    "                    #print(len(data.keys()))\n",
    "                    all_keys = list(data.keys())\n",
    "                    all_values = list(data.values())\n",
    "\n",
    "                    for i in range(len(all_keys)):\n",
    "                        #writer.writerow([all_keys[i], all_values[i]])\n",
    "                        writer.writerow([all_keys[i], json.dumps(all_values[i], ensure_ascii=False)])\n",
    "                \n",
    "                    \n",
    "                    \n",
    "\n",
    "                except (KeyError, IndexError, json.JSONDecodeError):\n",
    "                    # Skip lines that don't match the expected structure\n",
    "                    #print(\"Skipping line:\", line)\n",
    "                    skip_lines += 1\n",
    "                    #print(line[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"])\n",
    "                    \n",
    "                    pass\n",
    "    print(f\"Skipped {skip_lines} lines in {batch_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connectome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
